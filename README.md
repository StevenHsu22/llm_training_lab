# llm_training_lab (2023.10)
Try to train llama to achieve a specific task.

# Origin and Problem Definition
## Origin
1. Compared to models like ChatGPT and Bard, the responses of current open-source models are relatively unsatisfactory.
2. Llama TW performs relatively poorly on specific tasks, such as multiple-choice questions.
3. The hardware resource requirements for fine-tuning LLMs are excessively high.

## Target
Based on the above reasons, the goal is to fine-tune the model to achieve performance close to GPT on specific tasks, and to use quantization to lower the hardware resource requirements.

## Problem
It has been observed that there may be a "question generation" demand both in school exams and corporate training, such as:

- Generating Chinese exam questions
- Generating English exam questions
- Generating related questions based on an article (reading comprehension type questions)

# Collecting & Organizing Data

## Collecting Data

1. Download PDFs from the national elementary and middle school question banks (https://exam.naer.edu.tw/) and extract the text to pull out multiple-choice questions.

2. Use news articles to generate three multiple-choice questions based on the content, as generated by ChatGPT.

## Organizing
Organize into Fine-Tuning Prompt Format (Give different instructions based on subjects)
![image](https://github.com/user-attachments/assets/043f7bdd-08c9-4e60-aced-0be81d033305)

Data Collection Overview:
Chinese questions: 2130 questions
English questions: 2000 questions
reading questions: 18000 questions

# Training

## Models choices

Choose the currently most optimal open-source model, LLaMa2, and its derivatives.

| 13b | 7b |
|----------|----------|
| yentinglin/Taiwan-LLaMa-v1.0 | yentinglin/Taiwan-LLM-7B-v2.0.1-chat |
| meta-llama/Llama-2-13b-chat-hf | - |
| yentinglin/Taiwan-LLaMa-v1.0 & meta-llama/Llama-2-13b-chat-hf | - |

## finetune 

[samplecode](https://colab.research.google.com/drive/1rZvKty1E6oHSYdgjVdruYQqDbdaM0S51?authuser=1#scrollTo=p1A9B_sTSqee)

1. Load the model into memory using quantization.

2. Use the LoRA technique from Supervised Fine-tuning PEFT.

3. Provide data in prompt format.

4. Generate multiple-choice questions.

5. Combine the original model with adapters to achieve specific tasks.

6. Generate adapters.

## quantize

```
## terminal
git clone --depth 1 https://github.com/ggerganov/llama.cpp.git
pip install -r llama.cpp/requirements.txt
python convert.py Llama3-TAIDE-LX-8B-Chat-Alpha1 \
  --outfile Llama3-TAIDE-LX-8B-Chat-Alpha1.fp32.bin \
  --outtype f32 \
  --vocab-type bpe

cd llama.cpp/
make quantize
cd .. 
llama.cpp/quantize llama.cpp/TAIDE-LX-7B-Chat.fp16.bin llama.cpp/TAIDE-LX-7B-Chat.ggmlv3.Q5_K_M.gguf Q5_K_M
```

*If there are issues, you can modify the following make_ggml.py script yourself.*

```
def main(model, outname, outdir, quants, keep_fp16):
    ggml_version = "v3"

    if not os.path.isdir(model):
        print(f"Model not found at {model}. Downloading...")
        try:
            if outname is None:
                outname = model.split('/')[-1]
            model = snapshot_download(repo_id=model, cache_dir='../models/hf_cache')
        except Exception as e:
            raise Exception(f"Could not download the model: {e}")

    if outdir is None:
        outdir = f'../models/{outname}'

    if not os.path.isfile(f"{model}/config.json"):
        raise Exception(f"Could not find config.json in {model}")

    os.makedirs(outdir, exist_ok=True)

    print("Building llama.cpp")
    subprocess.run(f"cd llama.cpp && make quantize", shell=True, check=True)

    fp16 = f"{outdir}/{outname}.ggml{ggml_version}.fp16.bin"

    print(f"Making unquantised GGML at {fp16}")
    if not os.path.isfile(fp16):
        subprocess.run(f"python3 convert.py {model} --outtype f16 --outfile {fp16}", shell=True, check=True)
    else:
        print(f"Unquantised GGML already exists at: {fp16}")

    print("Making quants")
    for type in quants:
        outfile = f"{outdir}/{outname}.ggml{ggml_version}.{type}.bin"
        print(f"Making {type} : {outfile}")
        subprocess.run(f"../quantize {fp16} {outfile} {type}", shell=True, check=True)

    if not keep_fp16:
        os.remove(fp16)
```

# Final Model
https://huggingface.co/stevenhsu123/llama_finetune_test

